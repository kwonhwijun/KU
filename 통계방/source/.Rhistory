rownames(result) = 0:5
colnames(result) <- c("true", "lm", "ours", "glmnet")
print(round(result, 4))
# Soft Thresholding Operater
S <- function(z, lambda){
(z - lambda) * (z > lambda) + (z + lambda) * (z < -lambda) + 0 * (abs(z) < lambda)
}
# Coordinate Decent Alogorithm for Lasso
cd.lasso <- function(x, y, lambda){
z <- scale(x);
m <- attr(z, "scaled:center") # m: sample mean of x
s <- attr(z, "scaled:scale") # s: sample variance of x
u <- (y-mean(y)) # u : centered y
p = ncol(x)
# Initialization
beta <- coef(lm(u~z-1)) # Initalization
r<- u - z %*% beta # r : full residual
# Iteration
for (iter in 1:100){
new.beta <- beta
for (j in 1:p){
temp <- beta[j] + (matrix(z[,j], nrow = 1) %*% r)/n
new.beta[j] <- S(temp, lambda/s[j])
r <- r - (new.beta[j] - beta[j]) * z[,j]
}
delta <- max(abs(new.beta - beta))
if (delta < 1.0e-3) break
beta <- new.beta
}
beta <- new.beta/s
beta0 <- mean(y) - crossprod(beta, m)
c(beta0, beta)
}
sim_data <- function(n, p){
set.seed(1) ; n <- 100 ; p <- 5
x <- matrix(rnorm(n*p, 1, 1), n, p)
e <- rnorm(n ,0, 0.5)
true.beta <- rep(0, p+1) ; true.beta[1] <- 1
true.beta[2:(p+1)] <- c(rep(1, 3), rep(0, p-3))
y <- true.beta[1] + x %*% true.beta[-1] + e
list(x=x, y=y, beta= true.beta)
}
# debug
data = sim_data(100, 5)
x = data$x
y = data$y
true.beta = data$beta
est0 <- coef(lm(y~x))
est1 <- cd.lasso(x, y, lambda = 0.1)
est2 <- coef(glmnet::glmnet(x, y, lambda = 0.1, standardize = F))
result = cbind(true.beta, est0, est1, est2)
rownames(result) = 0:5
colnames(result) <- c("true", "lm", "ours", "glmnet")
print(round(result, 4))
# Soft Thresholding Operater
S <- function(z, lambda){
(z - lambda) * (z > lambda) + (z + lambda) * (z < -lambda) + 0 * (abs(z) < lambda)
}
# Coordinate Decent Alogorithm for Lasso
cd.lasso <- function(x, y, lambda){
z <- scale(x);
m <- attr(z, "scaled:center") # m: sample mean of x
s <- attr(z, "scaled:scale") # s: sample variance of x
u <- (y-mean(y)) # u : centered y
p = ncol(x)
# Initialization
beta <- coef(lm(u~z-1)) # Initalization
r<- u - z %*% beta # r : full residual
# Iteration
for (iter in 1:100){
new.beta <- beta
for (j in 1:p){
temp <- beta[j] + (z[,j] %*% r)/n
new.beta[j] <- S(temp, lambda/s[j])
r <- r - (new.beta[j] - beta[j]) * z[,j]
}
delta <- max(abs(new.beta - beta))
if (delta < 1.0e-3) break
beta <- new.beta
}
beta <- new.beta/s
beta0 <- mean(y) - crossprod(beta, m)
c(beta0, beta)
}
sim_data <- function(n, p){
set.seed(1) ; n <- 100 ; p <- 5
x <- matrix(rnorm(n*p, 1, 1), n, p)
e <- rnorm(n ,0, 0.5)
true.beta <- rep(0, p+1) ; true.beta[1] <- 1
true.beta[2:(p+1)] <- c(rep(1, 3), rep(0, p-3))
y <- true.beta[1] + x %*% true.beta[-1] + e
list(x=x, y=y, beta= true.beta)
}
# debug
data = sim_data(100, 5)
x = data$x
y = data$y
true.beta = data$beta
est0 <- coef(lm(y~x))
est1 <- cd.lasso(x, y, lambda = 0.1)
est2 <- coef(glmnet::glmnet(x, y, lambda = 0.1, standardize = F))
result = cbind(true.beta, est0, est1, est2)
rownames(result) = 0:5
colnames(result) <- c("true", "lm", "ours", "glmnet")
print(round(result, 4))
# Soft Thresholding Operater
S <- function(z, lambda){
(z - lambda) * (z > lambda) + (z + lambda) * (z < -lambda) + 0 * (abs(z) < lambda)
}
# Coordinate Decent Alogorithm for Lasso
cd.lasso <- function(x, y, lambda){
z <- scale(x);
m <- attr(z, "scaled:center") # m: sample mean of x
s <- attr(z, "scaled:scale") # s: sample variance of x
u <- (y-mean(y)) # u : centered y
p = ncol(x)
# Initialization
beta <- coef(lm(u~z-1)) # Initalization
r<- u - z %*% beta # r : full residual
# Iteration
for (iter in 1:100){
new.beta <- beta
for (j in 1:p){
temp <- beta[j] + (z[,j] %*% r)/n
new.beta[j] <- S(temp, lambda/s[j])
r <- r - (new.beta[j] - beta[j]) * z[,j]
}
delta <- max(abs(new.beta - beta))
if (delta < 1.0e-3) break
beta <- new.beta
}
beta <- new.beta/s
beta0 <- mean(y) - crossprod(beta, m)
c(beta0, beta)
}
sim_data <- function(n, p){
set.seed(1) ; n <- 100 ; p <- 5
x <- matrix(rnorm(n*p, 1, 1), n, p)
e <- rnorm(n ,0, 0.5)
true.beta <- rep(0, p+1) ; true.beta[1] <- 1
true.beta[2:(p+1)] <- c(rep(1, 3), rep(0, p-3))
y <- true.beta[1] + x %*% true.beta[-1] + e
list(x=x, y=y, beta= true.beta)
}
# debug
data = sim_data(100, 5)
x = data$x
y = data$y
true.beta = data$beta
est0 <- coef(lm(y~x))
est1 <- cd.lasso(x, y, lambda = 0.1)
est2 <- coef(glmnet::glmnet(x, y, lambda = 0.1, standardize = F))
result = cbind(true.beta, est0, est1, est2)
rownames(result) = 0:5
colnames(result) <- c("true", "lm", "ours", "glmnet")
print(round(result, 4))
# Soft Thresholding Operater
S <- function(z, lambda){
(z - lambda) * (z > lambda) + (z + lambda) * (z < -lambda) + 0 * (abs(z) < lambda)
}
# Coordinate Decent Alogorithm for Lasso
cd.lasso <- function(x, y, lambda){
z <- scale(x);
m <- attr(z, "scaled:center") # m: sample mean of x
s <- attr(z, "scaled:scale") # s: sample variance of x
u <- (y-mean(y)) # u : centered y
p = ncol(x)
# Initialization
beta <- coef(lm(u~z-1)) # Initalization
r<- u - z %*% beta # r : full residual
# Iteration
for (iter in 1:200){
new.beta <- beta
for (j in 1:p){
temp <- beta[j] + (z[,j] %*% r)/n
new.beta[j] <- S(temp, lambda/s[j])
r <- r - (new.beta[j] - beta[j]) * z[,j]
}
delta <- max(abs(new.beta - beta))
if (delta < 1.0e-3) break
beta <- new.beta
}
beta <- new.beta/s
beta0 <- mean(y) - crossprod(beta, m)
c(beta0, beta)
}
sim_data <- function(n, p){
set.seed(1) ; n <- 100 ; p <- 5
x <- matrix(rnorm(n*p, 1, 1), n, p)
e <- rnorm(n ,0, 0.5)
true.beta <- rep(0, p+1) ; true.beta[1] <- 1
true.beta[2:(p+1)] <- c(rep(1, 3), rep(0, p-3))
y <- true.beta[1] + x %*% true.beta[-1] + e
list(x=x, y=y, beta= true.beta)
}
# debug
data = sim_data(100, 5)
x = data$x
y = data$y
true.beta = data$beta
est0 <- coef(lm(y~x))
est1 <- cd.lasso(x, y, lambda = 0.1)
est2 <- coef(glmnet::glmnet(x, y, lambda = 0.1, standardize = F))
result = cbind(true.beta, est0, est1, est2)
rownames(result) = 0:5
colnames(result) <- c("true", "lm", "ours", "glmnet")
print(round(result, 4))
# Soft Thresholding Operater
S <- function(z, lambda){
(z - lambda) * (z > lambda) + (z + lambda) * (z < -lambda) + 0 * (abs(z) < lambda)
}
# Coordinate Decent Alogorithm for Lasso
cd.lasso <- function(x, y, lambda){
z <- scale(x);
m <- attr(z, "scaled:center") # m: sample mean of x
s <- attr(z, "scaled:scale") # s: sample variance of x
u <- (y-mean(y)) # u : centered y
p = ncol(x)
# Initialization
beta <- coef(lm(u~z-1)) # Initalization
r<- u - z %*% beta # r : full residual
# Iteration
for (iter in 1:200){
new.beta <- beta
for (j in 1:p){
temp <- beta[j] + (z[,j] %*% r)/n
new.beta[j] <- S(temp, lambda/s[j])
r <- r - (new.beta[j] - beta[j]) * z[,j]
}
delta <- max(abs(new.beta - beta))
if (delta < 1.0e-5) break
beta <- new.beta
}
beta <- new.beta/s
beta0 <- mean(y) - crossprod(beta, m)
c(beta0, beta)
}
sim_data <- function(n, p){
set.seed(1) ; n <- 100 ; p <- 5
x <- matrix(rnorm(n*p, 1, 1), n, p)
e <- rnorm(n ,0, 0.5)
true.beta <- rep(0, p+1) ; true.beta[1] <- 1
true.beta[2:(p+1)] <- c(rep(1, 3), rep(0, p-3))
y <- true.beta[1] + x %*% true.beta[-1] + e
list(x=x, y=y, beta= true.beta)
}
# debug
data = sim_data(100, 5)
x = data$x
y = data$y
true.beta = data$beta
est0 <- coef(lm(y~x))
est1 <- cd.lasso(x, y, lambda = 0.1)
est2 <- coef(glmnet::glmnet(x, y, lambda = 0.1, standardize = F))
result = cbind(true.beta, est0, est1, est2)
rownames(result) = 0:5
colnames(result) <- c("true", "lm", "ours", "glmnet")
print(round(result, 4))
# TEST
set.seed(1)
x = rnorm(3, mean = 0, sd = 1)
print(x)
# TEST
set.seed(1)
x = rnorm(3, mean = 0, sd = 1)
print(x)
# TEST
set.seed(1)
x = rnorm(3, mean = 0, sd = 1)
print(x)
# TEST
set.seed(1)
x = rnorm(3, mean = 0, sd = 1)
print(x)
# TEST
set.seed(1)
x = rnorm(3, mean = 0, sd = 1)
y = rnorm(4, mean = 0, sd = 2)
print(x)
print(y)
# TEST
set.seed(1)
x = rnorm(3, mean = 0, sd = 1)
y = rnorm(4, mean = 0, sd = 2)
print(x)
print(y)
# TEST
set.seed(1)
x = rnorm(3, mean = 0, sd = 1)
y = rnorm(4, mean = 0, sd = 2)
print(x)
print(y)
cd.elastic <- function(x, y, lambda, alpha){
# marginal standardization of x
z <- scale(x);
m <- attr(z, "scaled:center");
s <- attr(z, "scaled:scale")
# centering of y
u <- (y - mean(y))
# initialization
beta <- coef(lm(u ~ z - 1)); r <- u - z %*% beta
for (iter in 1:100){
new.beta <- beta
for (j in 1:ncol(x)) {
temp <- beta[j] + crossprod(z[,j], r)/nrow(x)
new.beta[j] <- S(temp, lambda*alpha)/(1+lambda*(1-alpha))
r <- r - (new.beta[j] - beta[j]) * z[,j]
}
delta <- max(abs(new.beta - beta))
if (delta < 1.0e-5) break
1
beta <- new.beta
}
beta <- new.beta/s ; beta0 <- mean(y) - crossprod(beta, m)
obj <- list(beta = c(beta0,beta))
return(obj)
}
prediction_error_elastic <- function(x.test,y.test,object){
y.pred <- x.test %*% matrix(object$beta[2:(ncol(x)+1)],ncol=1) +
object$beta[1]
mse <- mean((y.pred-y.test)ˆ2)
cd.elastic <- function(x, y, lambda, alpha){
# marginal standardization of x
z <- scale(x);
m <- attr(z, "scaled:center");
s <- attr(z, "scaled:scale")
# centering of y
u <- (y - mean(y))
# initialization
beta <- coef(lm(u ~ z - 1)); r <- u - z %*% beta
for (iter in 1:100){
new.beta <- beta
for (j in 1:ncol(x)) {
temp <- beta[j] + crossprod(z[,j], r)/nrow(x)
new.beta[j] <- S(temp, lambda*alpha)/(1+lambda*(1-alpha))
r <- r - (new.beta[j] - beta[j]) * z[,j]
}
delta <- max(abs(new.beta - beta))
if (delta < 1.0e-5) break
1
beta <- new.beta
}
beta <- new.beta/s ; beta0 <- mean(y) - crossprod(beta, m)
obj <- list(beta = c(beta0,beta))
return(obj)
}
prediction_error_elastic <- function(x.test,y.test,object){
y.pred <- x.test %*% matrix(object$beta[2:(ncol(x)+1)],ncol=1) +
object$beta[1]
mse <- mean((y.pred-y.test)ˆ2)
cd.elastic <- function(x, y, lambda, alpha){
# marginal standardization of x
z <- scale(x);
m <- attr(z, "scaled:center");
s <- attr(z, "scaled:scale")
# centering of y
u <- (y - mean(y))
# initialization
beta <- coef(lm(u ~ z - 1)); r <- u - z %*% beta
for (iter in 1:100){
new.beta <- beta
for (j in 1:ncol(x)) {
temp <- beta[j] + crossprod(z[,j], r)/nrow(x)
new.beta[j] <- S(temp, lambda*alpha)/(1+lambda*(1-alpha))
r <- r - (new.beta[j] - beta[j]) * z[,j]
}
delta <- max(abs(new.beta - beta))
if (delta < 1.0e-5) break
beta <- new.beta
}
beta <- new.beta/s ; beta0 <- mean(y) - crossprod(beta, m)
obj <- list(beta = c(beta0,beta))
return(obj)
}
prediction_error_elastic <- function(x.test,y.test,object){
y.pred <- x.test %*% matrix(object$beta[2:(ncol(x)+1)],ncol=1) +
object$beta[1]
mse <- mean((y.pred-y.test)ˆ2)
cd.elastic <- function(x, y, lambda, alpha){
# marginal standardization of x
z <- scale(x);
m <- attr(z, "scaled:center");
s <- attr(z, "scaled:scale")
# centering of y
u <- (y - mean(y))
# initialization
beta <- coef(lm(u ~ z - 1)); r <- u - z %*% beta
for (iter in 1:100){
new.beta <- beta
for (j in 1:ncol(x)) {
temp <- beta[j] + crossprod(z[,j], r)/nrow(x)
new.beta[j] <- S(temp, lambda*alpha)/(1+lambda*(1-alpha))
r <- r - (new.beta[j] - beta[j]) * z[,j]
}
delta <- max(abs(new.beta - beta))
if (delta < 1.0e-5) break
beta <- new.beta
}
beta <- new.beta/s ; beta0 <- mean(y) - crossprod(beta, m)
obj <- list(beta = c(beta0,beta))
return(obj)
}
prediction_error_elastic <- function(x.test,y.test,object){
y.pred <- x.test %*% matrix(object$beta[2:(ncol(x)+1)],ncol=1) +
object$beta[1]
mse <- mean((y.pred-y.test)ˆ2)
# 1-(a)
train <- matrix(scan("./train.txt"), 500, 51)
# 1-(a)
train <- matrix(scan("./train.txt"), 500, 51)
getwd()
setwd("~/Dropbox/KU/통계방/source")
getwd()
# 1-(a)
getwd()
train <- matrix(scan("./train.txt"), 500, 51)
test <- matrix(scan("./test.txt"), 500, 51)
x <- train[,-51]
y <- train[,51]
x.test <- test[,-51]
y.test <- test[,51]
# set soft-thresholding function
S <- function(z, lambda){
(z - lambda) * (z > lambda) + (z + lambda) * (z < -lambda) + 0 *
(abs(z) <= lambda)
}
# Coordinate decent algorithm for elastic net
cd.elastic <- function(x, y, lambda, alpha){
# marginal standardization of x
z <- scale(x);
m <- attr(z, "scaled:center");
s <- attr(z, "scaled:scale")
# centering of y
u <- (y - mean(y))
# initialization
beta <- coef(lm(u ~ z - 1)); r <- u - z %*% beta
for (iter in 1:100){
new.beta <- beta
for (j in 1:ncol(x)) {
temp <- beta[j] + crossprod(z[,j], r)/nrow(x)
new.beta[j] <- S(temp, lambda*alpha)/(1+lambda*(1-alpha))
r <- r - (new.beta[j] - beta[j]) * z[,j]
}
delta <- max(abs(new.beta - beta))
if (delta < 1.0e-5) break
beta <- new.beta
}
beta <- new.beta/s ; beta0 <- mean(y) - crossprod(beta, m)
obj <- list(beta = c(beta0,beta))
return(obj)
}
prediction_error_elastic <- function(x.test,y.test,object){
y.pred <- x.test %*% matrix(object$beta[2:(ncol(x)+1)],ncol=1) +
object$beta[1]
mse <- mean((y.pred-y.test)^2)
return(mse)
}
test.lambda2 <- seq(0,1,by=0.001)
n <- length(test.lambda2)
MSE_storage <- matrix(0,ncol=1,nrow=n)
for(i in 1:n){
CD.ELASTIC <- cd.elastic(x,y,lambda=test.lambda2[i],alpha=0.5)
MSE_storage[i,1] <- prediction_error_elastic(x.test,y.test,CD.ELASTIC)
}
best_lambda <- test.lambda2[which(MSE_storage == min(MSE_storage) ,
arr.ind = TRUE)[1]]
min(MSE_storage)
best_lambda
# Compare results with glmnet
library(glmnet)
lambda_values <- seq(0,1,by=0.001)
mse_list <- numeric(length(lambda_values))
for (i in seq_along(lambda_values)) {
model <- glmnet(x, y, alpha=0.5, lambda=lambda_values[i])
y_pred<- predict(model, newx=x.test)
mse_list[i] <- mean((y.test - y_pred)^2)
}
min_mse_idx <- which.min(mse_list)
lambda_min <- lambda_values[min_mse_idx]
min_mse <- mse_list[min_mse_idx]
cat("Lambda_Min:", lambda_min, "\nMin_MSE:", min_mse)
# MSE vs Lambda graph
plot(lambda_values, mse_list, col = "blue", lwd = 2,
xlab = "Lambda", ylab = "Test MSE", main = "Test MSE vs. Lambda")
points(lambda_min, min_mse, col = "red", pch = 19, cex = 1.5)
text(lambda_min, min_mse, labels = sprintf("Min MSE at Lambda=%.4f", lambda_min),
pos = 4, col = "red")
##################################################################################
# 1-(b)
# with given dataset
# First coefficient is an intercept
cd.elastic(x,y,0.038,0.5)$beta[cd.elastic(x,y,0.038,0.5)$beta != 0]
model<-glmnet(x,y,alpha=0.5,lambda = 0.038)
# same result with "cd.elastic" function
# this result does not report intercept
model$beta[model$beta != 0]
library(MASS)
generate_data <- function(n, p, nu) {
set.seed(2024020409)
mu <- rep(0, p)
sigma <- outer(1:p, 1:p, FUN = function(i, j) nu^(abs(i-j)))
x <- mvrnorm(n=n, mu=mu, Sigma=sigma)
beta0 = 1
beta = c(rep(1,3), rep(0, p-3))
mu_x <- exp(beta0 + x %*% beta)
e <- rnorm(n, 0, 0.5)
y <- rpois(n, mu_x) + e
return(list(x = x, y = y))
}
set.seed(2024020409)
train_data <- generate_data(500, 50, 0.7)
test_data <- generate_data(500, 50, 0.7)
